# ───────── configs/config.yaml ─────────
# Base entry-point. Everything else is pulled in via `defaults`.

hydra:
  job:
    chdir: false

defaults:
  - datamodule: default
  - optimizer:  adamw             # configs/optimizer/adamw.yaml
  - attentive_rnn: gla            # configs/attentive_rnn/gla.yaml

seed_everything: 123

trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  precision: bf16-mixed
  strategy: ddp
  devices: 1
  use_distributed_sampler: false
  max_epochs: 100
  gradient_clip_val: 1.0
  val_check_interval: 0.50
  limit_train_batches: 10000
  accumulate_grad_batches: 1
  log_every_n_steps: 1

  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    project: lina
    name: llm_nemo-GLA-d1024-l6-lr2e-4
    save_dir: /data/guichoux/LINA

  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val_loss
      dirpath: /data/guichoux/LINA/checkpoints/${trainer.logger.name}
      filename: checkpoint_{epoch}_{step}_{val_loss:.4f}
      save_top_k: 1
      save_last: true
    - _target_: pytorch_lightning.callbacks.ModelSummary
      max_depth: 2

model:
  _target_: packages.lina.src.lina.lina_trainer.TrainLina
  
  n_warmup_steps: 5000
  n_training_steps: 300000
  learning_rate: 2e-4

  n_codebook: 4096
  n_special_token_in: 3
  n_special_token_out: 3
  n_txt_vocab: 256
  d_model: 1024
  quant_layer: [0]

  txt_encoder:
    _target_: packages.common.src.common.blocks.encoder.TextEncoder
    dim: ${model.d_model}
    heads: 4
    n_layers: 6
    dropout: 0.1

  attentive_rnn: ${attentive_rnn}   # pulled from defaults
